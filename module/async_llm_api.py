"""
éåŒæœŸå¯¾å¿œç‰ˆ LLM APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
æ—¢å­˜ã® learning_plannner ã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã—ã€éåŒæœŸå‡¦ç†ã«å¯¾å¿œã—ã¾ã™ã€‚
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from openai import AsyncOpenAI, OpenAI
from dotenv import load_dotenv
from prompt.prompt import system_prompt
from module.llm_api import learning_plannner


class AsyncLearningPlanner(learning_plannner):
    """
    éåŒæœŸå¯¾å¿œç‰ˆã® learning_plannner
    æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
    """
    
    def __init__(self, pool_size: int = 5):
        """
        åˆæœŸåŒ–
        
        Args:
            pool_size: ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ—ãƒ¼ãƒ«æ•°
        """
        super().__init__()
        
        # éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
        # éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        self.async_client = AsyncOpenAI(
            api_key=api_key,
            timeout=30.0,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’30ç§’ã«è¨­å®š
            max_retries=2   # ãƒªãƒˆãƒ©ã‚¤ã‚’2å›ã«åˆ¶é™
        )
        
        # ã‚»ãƒãƒ•ã‚©ã§APIåŒæ™‚å‘¼ã³å‡ºã—æ•°ã‚’åˆ¶é™
        self.semaphore = asyncio.Semaphore(pool_size)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ç”¨
        self.request_count = 0
        self.total_response_time = 0.0
    
    async def generate_response_async(self, messages: List[Dict[str, Any]]) -> str:
        """
        å¯¾è©±å±¥æ­´ã‚’è€ƒæ…®ã—ã¦LLMã‹ã‚‰éåŒæœŸã§å¿œç­”ã‚’ç”Ÿæˆ
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
            
        Returns:
            LLMã‹ã‚‰ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        import time
        start_time = time.time()
        
        try:
            # ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ã¦åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
            async with self.semaphore:
                response = await self.async_client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=2000
                )
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
                response_time = time.time() - start_time
                self.request_count += 1
                self.total_response_time += response_time
                
                if self.request_count % 10 == 0:  # 10ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ãƒ­ã‚°
                    avg_time = self.total_response_time / self.request_count
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.info(f"ğŸ“Š OpenAI APIå¹³å‡å¿œç­”æ™‚é–“: {avg_time:.2f}ç§’")
                
                return response.choices[0].message.content
                
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ OpenAI APIéåŒæœŸå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŒæœŸç‰ˆã‚’éåŒæœŸã§å®Ÿè¡Œ
            return await asyncio.to_thread(
                self.generate_response,
                messages
            )
    
    async def generate_response_streaming(
        self, 
        messages: List[Dict[str, Any]], 
        callback: Optional[callable] = None
    ):
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®éåŒæœŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
            callback: ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‘¼ã°ã‚Œã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
            
        Yields:
            ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒãƒ£ãƒ³ã‚¯
        """
        try:
            async with self.semaphore:
                stream = await self.async_client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=2000,
                    stream=True
                )
                
                full_content = ""
                async for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        full_content += content
                        
                        if callback:
                            await callback(content)
                        
                        yield content
                
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def batch_generate_responses(
        self, 
        message_sets: List[List[Dict[str, Any]]]
    ) -> List[str]:
        """
        è¤‡æ•°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚»ãƒƒãƒˆã‚’ä¸¦åˆ—ã§å‡¦ç†
        
        Args:
            message_sets: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚»ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        tasks = [
            self.generate_response_async(messages)
            for messages in message_sets
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        responses = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ (index={i}): {result}")
                responses.append("ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            else:
                responses.append(result)
        
        return responses
    
    async def generate_with_fallback(
        self, 
        messages: List[Dict[str, Any]], 
        fallback_model: Optional[str] = "gpt-3.5-turbo"
    ) -> str:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
            fallback_model: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            LLMã‹ã‚‰ã®å¿œç­”
        """
        try:
            # ã¾ãšãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§è©¦è¡Œ
            return await self.generate_response_async(messages)
            
        except Exception as primary_error:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"âš ï¸ ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨: {primary_error}")
            
            if fallback_model:
                try:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã§å†è©¦è¡Œ
                    async with self.semaphore:
                        response = await self.async_client.chat.completions.create(
                            model=fallback_model,
                            messages=messages,
                            temperature=0.7,
                            max_tokens=1500  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å°‘ã—åˆ¶é™
                        )
                        return response.choices[0].message.content
                        
                except Exception as fallback_error:
                    logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚‚ã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    raise
            else:
                raise primary_error
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—
        
        Returns:
            ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®Dict
        """
        if self.request_count == 0:
            return {
                "total_requests": 0,
                "average_response_time": 0,
                "active_connections": self.semaphore._value if hasattr(self.semaphore, '_value') else None
            }
        
        return {
            "total_requests": self.request_count,
            "average_response_time": self.total_response_time / self.request_count,
            "active_connections": self.semaphore._value if hasattr(self.semaphore, '_value') else None
        }


# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç®¡ç†
_async_llm_instance: Optional[AsyncLearningPlanner] = None

def get_async_llm_client(pool_size: int = 5) -> AsyncLearningPlanner:
    """
    éåŒæœŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚’å–å¾—
    
    Args:
        pool_size: ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºï¼ˆåˆå›ã®ã¿æœ‰åŠ¹ï¼‰
        
    Returns:
        AsyncLearningPlannerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    global _async_llm_instance
    
    if _async_llm_instance is None:
        _async_llm_instance = AsyncLearningPlanner(pool_size=pool_size)
    
    return _async_llm_instance