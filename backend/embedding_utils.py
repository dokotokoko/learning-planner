"""
åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆã¨æ¤œç´¢ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
Phase 2ã§ä½¿ç”¨ã•ã‚Œã‚‹é«˜åº¦ãªæ¤œç´¢æ©Ÿèƒ½
"""
import os
import json
import hashlib
import logging
import asyncio
from typing import List, Dict, Optional, Any, Tuple
import numpy as np
from datetime import datetime, timezone
import aiohttp
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    id: int
    text: str
    score: float
    metadata: Dict[str, Any]

class EmbeddingClient:
    """
    åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    OpenAI, Cohere, ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ
    """
    
    def __init__(self, provider: str = "openai", api_key: Optional[str] = None):
        self.provider = provider.lower()
        self.api_key = api_key or os.environ.get(f"{provider.upper()}_API_KEY")
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.model_configs = {
            "openai": {
                "model": os.environ.get("EMBEDDING_MODEL", "text-embedding-ada-002"),
                "dim": 1536,
                "endpoint": "https://api.openai.com/v1/embeddings"
            },
            "cohere": {
                "model": "embed-english-v2.0",
                "dim": 4096,
                "endpoint": "https://api.cohere.ai/v1/embed"
            }
        }
        
        self.config = self.model_configs.get(self.provider)
        if not self.config:
            raise ValueError(f"Unsupported provider: {self.provider}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        logger.info(f"ğŸ“Š EmbeddingClientåˆæœŸåŒ–: {self.provider}/{self.config['model']}")
    
    async def generate_embedding(self, text: str, use_cache: bool = True) -> np.ndarray:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        """
        if not text:
            return np.zeros(self.config["dim"])
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        text_hash = hashlib.sha256(text.encode()).hexdigest()
        if use_cache and text_hash in self.cache:
            self.cache_hits += 1
            return self.cache[text_hash]
        
        self.cache_misses += 1
        
        try:
            if self.provider == "openai":
                embedding = await self._generate_openai_embedding(text)
            elif self.provider == "cohere":
                embedding = await self._generate_cohere_embedding(text)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé–‹ç™ºç”¨ï¼‰
                logger.warning(f"âš ï¸ é–‹ç™ºç”¨: ãƒ©ãƒ³ãƒ€ãƒ åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ")
                embedding = np.random.randn(self.config["dim"])
                embedding = embedding / np.linalg.norm(embedding)  # æ­£è¦åŒ–
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if use_cache:
                self.cache[text_hash] = embedding
            
            return embedding
            
        except Exception as e:
            logger.error(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(self.config["dim"])
    
    async def _generate_openai_embedding(self, text: str) -> np.ndarray:
        """OpenAI APIã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        if not self.api_key:
            logger.warning("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return np.random.randn(self.config["dim"])
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "input": text,
            "model": self.config["model"]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.config["endpoint"],
                headers=headers,
                json=payload
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    embedding = data["data"][0]["embedding"]
                    return np.array(embedding)
                else:
                    error_text = await response.text()
                    raise Exception(f"OpenAI API error: {response.status} - {error_text}")
    
    async def _generate_cohere_embedding(self, text: str) -> np.ndarray:
        """Cohere APIã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        if not self.api_key:
            logger.warning("âš ï¸ Cohere APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return np.random.randn(self.config["dim"])
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "texts": [text],
            "model": self.config["model"]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.config["endpoint"],
                headers=headers,
                json=payload
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    embedding = data["embeddings"][0]
                    return np.array(embedding)
                else:
                    error_text = await response.text()
                    raise Exception(f"Cohere API error: {response.status} - {error_text}")
    
    async def generate_batch_embeddings(
        self,
        texts: List[str],
        batch_size: int = 100
    ) -> List[np.ndarray]:
        """
        ãƒãƒƒãƒã§åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        """
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = await asyncio.gather(
                *[self.generate_embedding(text) for text in batch]
            )
            embeddings.extend(batch_embeddings)
        
        return embeddings
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        if vec1.shape != vec2.shape:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“")
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            "cache_size": len(self.cache),
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": hit_rate
        }


class SemanticSearch:
    """
    æ„å‘³çš„é¡ä¼¼æ¤œç´¢ã¨MMRãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
    """
    
    def __init__(
        self,
        embedding_client: EmbeddingClient,
        supabase_client=None
    ):
        self.embedding_client = embedding_client
        self.supabase = supabase_client
        
        # MMRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.mmr_lambda = float(os.environ.get("MMR_LAMBDA", "0.7"))
        self.recency_decay = float(os.environ.get("RECENCY_DECAY", "0.95"))
        
        logger.info(f"ğŸ” SemanticSearchåˆæœŸåŒ–: MMR Î»={self.mmr_lambda}")
    
    async def search(
        self,
        query: str,
        conversation_id: str,
        k: int = 10,
        min_similarity: float = 0.5,
        exclude_recent_n: int = 10,
        use_mmr: bool = True
    ) -> List[SearchResult]:
        """
        æ„å‘³çš„é¡ä¼¼æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            conversation_id: ä¼šè©±ID
            k: å–å¾—ã™ã‚‹çµæœæ•°
            min_similarity: æœ€å°é¡ä¼¼åº¦é–¾å€¤
            exclude_recent_n: é™¤å¤–ã™ã‚‹ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
            use_mmr: MMRãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        logger.info(f"ğŸ” æ¤œç´¢é–‹å§‹: '{query[:50]}...' (k={k})")
        
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = await self.embedding_client.generate_embedding(query)
        
        # Supabaseã‹ã‚‰é¡ä¼¼æ¤œç´¢ï¼ˆPhase 2ã§å®Ÿè£…ï¼‰
        if self.supabase:
            results = await self._search_with_supabase(
                query_embedding,
                conversation_id,
                k * 2,  # MMRç”¨ã«å¤šã‚ã«å–å¾—
                min_similarity,
                exclude_recent_n
            )
        else:
            # ãƒ¢ãƒƒã‚¯çµæœï¼ˆé–‹ç™ºç”¨ï¼‰
            results = self._get_mock_results(k)
        
        # MMRãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if use_mmr and len(results) > k:
            results = self._mmr_rerank(query_embedding, results, k)
        
        # Top-kå–å¾—
        results = results[:k]
        
        logger.info(f"âœ… æ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®çµæœ")
        
        return results
    
    async def _search_with_supabase(
        self,
        query_embedding: np.ndarray,
        conversation_id: str,
        k: int,
        min_similarity: float,
        exclude_recent_n: int
    ) -> List[SearchResult]:
        """Supabaseã§é¡ä¼¼æ¤œç´¢ï¼ˆPhase 2ã§å®Ÿè£…ï¼‰"""
        # TODO: pgvectorã‚’ä½¿ã£ãŸæ¤œç´¢ã‚’å®Ÿè£…
        return []
    
    def _get_mock_results(self, k: int) -> List[SearchResult]:
        """ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæœï¼ˆé–‹ç™ºç”¨ï¼‰"""
        results = []
        for i in range(k):
            results.append(SearchResult(
                id=i,
                text=f"éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i+1}: ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚",
                score=0.9 - i * 0.1,
                metadata={"created_at": datetime.now(timezone.utc).isoformat()}
            ))
        return results
    
    def _mmr_rerank(
        self,
        query_embedding: np.ndarray,
        candidates: List[SearchResult],
        k: int
    ) -> List[SearchResult]:
        """
        Maximum Marginal Relevance (MMR) ã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
        é–¢é€£æ€§ã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹
        """
        if not candidates:
            return []
        
        selected = []
        remaining = list(candidates)
        
        # æœ€åˆã®è¦ç´ ã¯æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’é¸æŠ
        first = max(remaining, key=lambda x: x.score)
        selected.append(first)
        remaining.remove(first)
        
        # æ®‹ã‚Šã‚’MMRã‚¹ã‚³ã‚¢ã§é¸æŠ
        while len(selected) < k and remaining:
            mmr_scores = []
            
            for candidate in remaining:
                # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ—¢ã«è¨ˆç®—æ¸ˆã¿ï¼‰
                relevance = candidate.score
                
                # å¤šæ§˜æ€§ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆé¸æŠæ¸ˆã¿ã¨ã®æœ€å¤§é¡ä¼¼åº¦ï¼‰
                max_sim = 0.0
                for selected_item in selected:
                    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ãƒ†ã‚­ã‚¹ãƒˆã®é‡è¤‡åº¦ã‚’ä½¿ç”¨
                    # Phase 2ã§ã¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’ä½¿ç”¨
                    overlap = len(set(candidate.text.split()) & set(selected_item.text.split()))
                    sim = overlap / max(len(candidate.text.split()), 1)
                    max_sim = max(max_sim, sim)
                
                # MMRã‚¹ã‚³ã‚¢è¨ˆç®—
                mmr_score = self.mmr_lambda * relevance - (1 - self.mmr_lambda) * max_sim
                mmr_scores.append((candidate, mmr_score))
            
            # æœ€é«˜MMRã‚¹ã‚³ã‚¢ã®è¦ç´ ã‚’é¸æŠ
            best_candidate = max(mmr_scores, key=lambda x: x[1])[0]
            selected.append(best_candidate)
            remaining.remove(best_candidate)
        
        return selected
    
    def detect_topic_switch(
        self,
        current_embedding: np.ndarray,
        recent_embeddings: List[np.ndarray],
        threshold: float = None
    ) -> bool:
        """
        ãƒˆãƒ”ãƒƒã‚¯åˆ‡ã‚Šæ›¿ãˆã‚’æ¤œå‡º
        ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é¡ä¼¼åº¦ãŒé–¾å€¤ä»¥ä¸‹ã®å ´åˆTrue
        """
        if not recent_embeddings:
            return False
        
        threshold = threshold or float(os.environ.get("TOPIC_TAU", "0.78"))
        
        # ç›´è¿‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¹³å‡åŸ‹ã‚è¾¼ã¿
        recent_avg = np.mean(recent_embeddings, axis=0)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarity = self.embedding_client.cosine_similarity(current_embedding, recent_avg)
        
        is_switch = similarity < threshold
        
        if is_switch:
            logger.info(f"ğŸ”„ ãƒˆãƒ”ãƒƒã‚¯åˆ‡ã‚Šæ›¿ãˆæ¤œå‡º: é¡ä¼¼åº¦={similarity:.3f} < {threshold}")
        
        return is_switch
    
    async def apply_recency_boost(
        self,
        results: List[SearchResult],
        decay_factor: float = None
    ) -> List[SearchResult]:
        """
        æ™‚é–“æ¸›è¡°ã‚’é©ç”¨ã—ã¦æœ€è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ–ãƒ¼ã‚¹ãƒˆ
        """
        decay_factor = decay_factor or self.recency_decay
        
        now = datetime.now(timezone.utc)
        
        for result in results:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œæˆæ™‚åˆ»ã‚’å–å¾—
            created_at_str = result.metadata.get("created_at")
            if created_at_str:
                created_at = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
                
                # çµŒéæ™‚é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰
                hours_ago = (now - created_at).total_seconds() / 3600
                
                # æŒ‡æ•°æ¸›è¡°ã‚’é©ç”¨
                recency_factor = decay_factor ** hours_ago
                
                # ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´
                result.score *= recency_factor
        
        # ã‚¹ã‚³ã‚¢ã§å†ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.score, reverse=True)
        
        return results